{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 GA-DSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 1,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683255529,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import (WordCloud, get_single_color_func)\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 7,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264612,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "Countries = {\"SG\":\"Singapore\",\"US\":\"United States\",\"MY\":\"Malaysia\",\"UK\":\"United Kingdom\",\\\n",
    "             \"AU\":\"Australia\",\"CN\":\"Canada\"}\n",
    "Target_cities= {'US':['New York', 'Chicago', 'San Francisco', 'Austin', 'Seattle',\n",
    "                  'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas',\n",
    "                  'Houston'],\n",
    "                'SG':[\"Singapore\"],\n",
    "                'MY':['Kuala Lumpur'],\n",
    "                \"UK\":[\"London\", \"Newcastle\",\"Sheffield\",\"Bristol\",\"Manchester\",\"Glasgow\",\"Edinburgh\",\"Birmingham\",\"Liverpool\",\n",
    "                      \"Aberdeen\",\"Nottingham\",\"Belfast\",\"Cardiff\",\"Cambridge\",\"Oxford\"],\n",
    "                \"AU\" :[\"Sydney\",\"Melbourne\",\"Bisbane\",\"Perth\"],\n",
    "                \"CN\" :[\"Toronto\",\"Montreal\",\"Vancouver\",\"Quebec\"]\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 8,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264635,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "URL = {\"SG\":\"https://www.indeed.com.sg/jobs\",\n",
    "       \"US\":\"https://www.indeed.com/jobs\",\n",
    "       \"MY\":\"https://www.indeed.com.my/jobs\",\n",
    "       \"UK\":\"https://www.indeed.co.uk/jobs\",\n",
    "       \"AU\":\"https://au.indeed.com/jobs\",\n",
    "       \"CN\":\"https://ca.indeed.com/jobs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 9,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264647,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "max_results_per_city = 300\n",
    "#put data scientist as a placeholder, will be filled with the list of jobs later\n",
    "parameters = {'q': 'data scientist', 'radius': '100', 'start':1}\n",
    "#list of jobs\n",
    "jobs = ['data scientist','data analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 10,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264685,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "def scrape_page_to_df(url, url_params,country):\n",
    "    \"\"\"\n",
    "    extract information from a results page and save to an existing csv\n",
    "    :param url: url template\n",
    "    :param url_params: a dictionary to feed to params argument in requests.get (based on the parameters I defined above, and I'll make a wrapper to do this below)\n",
    "    :return: a pandas dataframe containing the extracted information\n",
    "    \"\"\"\n",
    "    # create a empty dictionary to store extracted information\n",
    "    scraped_data = {'location': [],\n",
    "                  'company': [],\n",
    "                  'title': [],\n",
    "                  'salary': [],\n",
    "                  'description': [],\n",
    "                  'review': [],\n",
    "                  'star': [],\n",
    "                  'country':[]\n",
    "                  }\n",
    "\n",
    "    html = requests.get(url, params=url_params)\n",
    "\n",
    "    # make sure the response status is ok\n",
    "    assert html.status_code == requests.codes.ok\n",
    "\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "  #helper function to extract results\n",
    "\n",
    "    def extract_results(soup):\n",
    "        return soup.find_all('div', class_='result')\n",
    "\n",
    "    results = extract_results(soup)\n",
    "\n",
    "  #helper functions to extract information\n",
    "    def extract_location(result):\n",
    "        \"\"\"extract job location\"\"\"\n",
    "        try:\n",
    "            location = result.find('span', class_='location').get_text().strip()\n",
    "            return location\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_company(result):\n",
    "        \"\"\"extract the name of the company\"\"\"\n",
    "        try:\n",
    "            company = result.find('span', class_='company').get_text().strip()\n",
    "            return company\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extract_title(result):\n",
    "        \"\"\"extract the job title\"\"\"\n",
    "        try:\n",
    "            title = result.find('a', attrs={'data-tn-element': \"jobTitle\"}).get('title')\n",
    "            return title\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_salary(result):\n",
    "        \"\"\"extract the salary\"\"\"\n",
    "        try:\n",
    "            salary = result.find('td', class_='snip').\\\n",
    "            find('span', class_='no-wrap').\\\n",
    "            get_text().strip()\n",
    "            return salary\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_description(result):\n",
    "        \"\"\"extract job description snippet\"\"\"\n",
    "        try:\n",
    "            description = result.find('span', class_='summary').get_text().strip()\n",
    "            return description\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_review(result):\n",
    "        \"\"\"extract the number of reviews for the company\"\"\"\n",
    "        try:\n",
    "            review = result.find('a', attrs={'data-tn-element': \"reviewStars\"})\n",
    "            review = review.find('span', class_=\"slNoUnderline\")\n",
    "            review = review.get_text().strip()\n",
    "            # extract only the number\n",
    "            review = review.replace(',', '').replace(' reviews', '')\n",
    "            return review\n",
    "        except:\n",
    "            return None            \n",
    "\n",
    "    \n",
    "    def extract_star(result):\n",
    "        \"\"\"extract a number (width) that is proportional to the number of stars\n",
    "        shown for the company\"\"\"\n",
    "        try:\n",
    "            # the 'style' attribute dictates how many stars are filled with color\n",
    "            star = result.find('span', class_='rating').get('style')\n",
    "            # extract only the number\n",
    "            star = star.replace('width:', '').replace('px', '')\n",
    "            return star\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # append extracted info to the correspond list\n",
    "    for result in results:\n",
    "        scraped_data['location'].append(extract_location(result))\n",
    "        scraped_data['company'].append(extract_company(result))\n",
    "        scraped_data['title'].append(extract_title(result))\n",
    "        scraped_data['salary'].append(extract_salary(result))\n",
    "        scraped_data['description'].append(extract_description(result))\n",
    "        scraped_data['review'].append(extract_review(result))\n",
    "        scraped_data['star'].append(extract_star(result))\n",
    "        scraped_data['country'].append(country)\n",
    "\n",
    "      # convert the dictionary to a pandas dataframe and returns it\n",
    "    return pd.DataFrame(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 11,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264722,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"remove duplicates and returns a new df\"\"\"\n",
    "    \n",
    "    nrows_before = df.shape[0]\n",
    "    df.drop_duplicates(subset=['company', 'country','description',\n",
    "                               'location', 'salary', 'title'],\n",
    "                       keep='last', inplace=True)\n",
    "    nrows_after = df.shape[0]\n",
    "    \n",
    "    print('{} rows remain after removing duplicates from {} rows.'.format(\n",
    "        nrows_after, nrows_before))\n",
    "    print('{} rows have salary info; {} rows have yearly salary info.'.format(\n",
    "      df.salary.notnull().sum(), df.salary.str.contains('year').sum()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 12,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683264731,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [],
   "source": [
    "def scrapper(CountryCode):\n",
    "    print('Current system time: {}'.format(time.ctime()))\n",
    "  \n",
    "    # scrape data and save to dataframe\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Retrieve Parameters to scrape with based on input of Country Code\n",
    "    url = URL[CountryCode]\n",
    "    locations = Target_cities[CountryCode]\n",
    "    country = Countries[CountryCode]\n",
    "    \n",
    "    #Create an empty place holder df, search through every location in that country, but only 1 results, just to get the title and columns\n",
    "    df = scrape_page_to_df(url,parameters,country)\n",
    "    \n",
    "    \n",
    "    for loc in locations:\n",
    "        for j in jobs:\n",
    "            for start in range(0, max_results_per_city, 10):\n",
    "            \n",
    "              \n",
    "                url_params = parameters.copy()\n",
    "                #update the job with the target job that we want, city for target city that we are looking for and start refers to the current page number being scrapped\n",
    "                url_params.update({'l': loc,'q': j, 'start': start})\n",
    "\n",
    "\n",
    "                #insert code to put the scrap stuff into a df here, after each round of loop, concat into a df\n",
    "                df = pd.concat([df,scrape_page_to_df(url, url_params,country)],axis=0)\n",
    "              \n",
    "        print('Finished scraping {}'.format(loc))\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print('Scraping run time: {:.1f} minutes'.format(total_time))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove duplicates\n",
    "    df = remove_duplicates(df)\n",
    "    print('Script finished at {}\\n'.format(time.ctime()))\n",
    "    \n",
    "    #returns the final df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": null,
    "executionInfo": {
     "content": {
      "execution_count": 13,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1508683617720,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Tue Oct 24 10:01:59 2017\n",
      "Finished scraping Singapore\n",
      "Scraping run time: 0.8 minutes\n",
      "738 rows remain after removing duplicates from 1506 rows.\n",
      "26 rows have salary info; 0 rows have yearly salary info.\n",
      "Script finished at Tue Oct 24 10:02:47 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SG = scrapper('SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Tue Oct 24 13:28:27 2017\n",
      "Finished scraping Sydney\n",
      "Finished scraping Melbourne\n",
      "Scraping run time: 4.6 minutes\n",
      "780 rows remain after removing duplicates from 1208 rows.\n",
      "117 rows have salary info; 89 rows have yearly salary info.\n",
      "Script finished at Tue Oct 24 13:33:01 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AU = scrapper('AU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Tue Oct 24 13:33:31 2017\n",
      "Finished scraping London\n",
      "Scraping run time: 2.6 minutes\n",
      "520 rows remain after removing duplicates from 610 rows.\n",
      "122 rows have salary info; 107 rows have yearly salary info.\n",
      "Script finished at Tue Oct 24 13:36:05 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "UK = scrapper('UK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": null,
    "executionInfo": {
     "content": {
      "status": "aborted"
     },
     "timestamp": 1508683902987,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Tue Oct 24 10:17:10 2017\n",
      "Finished scraping Hong Kong\n",
      "Scraping run time: 0.4 minutes\n",
      "361 rows remain after removing duplicates from 607 rows.\n",
      "9 rows have salary info; 0 rows have yearly salary info.\n",
      "Script finished at Tue Oct 24 10:17:35 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HK= scrapper('HK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cellView": null,
    "executionInfo": {
     "content": {
      "status": "aborted"
     },
     "timestamp": 1508683902976,
     "user": {
      "color": "#1FA15D",
      "displayName": "Nasrudin Salim",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "08405076575781058120",
      "photoUrl": "//lh5.googleusercontent.com/-_losW-TOCfY/AAAAAAAAAAI/AAAAAAAAs2I/-pEq-D9OnXc/s50-c-k-no/photo.jpg",
      "sessionId": "2b925d78e3c2f27b",
      "userId": "108356546527876522753"
     },
     "user_tz": -480
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system time: Tue Oct 24 12:02:38 2017\n",
      "Finished scraping Seattle\n",
      "Finished scraping Boston\n",
      "Finished scraping Austin\n",
      "Scraping run time: 4.3 minutes\n",
      "1533 rows remain after removing duplicates from 1831 rows.\n",
      "113 rows have salary info; 58 rows have yearly salary info.\n",
      "Script finished at Tue Oct 24 12:06:58 2017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "US = scrapper('US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MY = scrapper('MY')\n",
    "CN = scrapper('CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AU.to_pickle('./data/AU.pkl')\n",
    "SG.to_pickle('./data/SG.pkl')\n",
    "US.to_pickle('./data/US.pkl')\n",
    "UK.to_pickle('./data/UK.pkl')\n",
    "HK.to_pickle('./data/HK.pkl')\n",
    "MY.to_pickle('./data/MY.pkl')\n",
    "CN.to_pickle('./data/CN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AU.to_csv('./data/AU.csv')\n",
    "SG.to_csv('./data/SG.csv')\n",
    "US.to_csv('./data/US.csv')\n",
    "UK.to_csv('./data/UK.csv')\n",
    "HK.to_csv('./data/HK.csv')\n",
    "MY.to_csv('./data/MY.csv')\n",
    "CN.to_csv('./data/CN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft = pd.concat([SG, HK, US, AU, UK, MY, CN], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft.to_pickle('./data/total.pickle')"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
